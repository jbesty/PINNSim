{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a PINN for PINNSim\n",
    "\n",
    "The following notebook describes the basic elements of the workflow of training a PINN. The following structure mimics `pinnsim.learning_functions.workflow`, however, some sections of the workflow are simplified here for illustration purposes. At the end of this notebook, we explain in brief how `pinnsim.learning_functions.workflow` can be used on a high level, for the details we refer to the function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pinnsim import LEARNING_DATA_PATH\n",
    "from pinnsim.configurations.hyperparameter_configs import (\n",
    "    convert_sweep_config_to_run_config,\n",
    "    default_hyperparameter_setup,\n",
    ")\n",
    "from pinnsim.learning_functions.loss_normed_state import LossNormedState\n",
    "from pinnsim.learning_functions.setup_functions import (\n",
    "    setup_dataset,\n",
    "    setup_nn_model,\n",
    "    setup_optimiser,\n",
    "    setup_schedulers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by defining a config file that contains all relevant information, e.g., the generator name (`generator_name`) or the number of neurons per hidden layer (`hidden_layer_size`). Ideally, all variations to the training setup can be adjusted in the config file as this allows us to easily modify and track the training setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = default_hyperparameter_setup()\n",
    "config = convert_sweep_config_to_run_config(sweep_config=sweep_config)\n",
    "\n",
    "print(config.generator_name)\n",
    "print(config.hidden_layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Based on this config file, we load the datasets and the component model, i.e., the generator model. We will use `dataset_training` and `dataset_collocation` to optimise the NN parameters, while `dataset_validation` is used to track the training process and select the best performing model. After the training `dataset_testing` is used to asses the performance in an unbiased way. As the dataset are simulated, we want to avoid to re-simulate the datasets for every training run. Therefore, we once simulate large datasets, store them in `LEARNING_DATA_PATH` (specified in `pinnsim_utils.__init__.py`) and subsequently only sample from those. For more details see `setup_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dataset_training,\n",
    "    dataset_validation,\n",
    "    dataset_testing,\n",
    "    dataset_collocation,\n",
    "    component_model,\n",
    ") = setup_dataset(config=config, data_path=LEARNING_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are constructed similar to datasets from `torch.utils.data` (see `pinnsim.dataset_functions.dataset_object.py`). This allows indexing the dataset as shown below. The returned tuple corresponds to \n",
    "- time $t$\n",
    "- initial_state $x_0$\n",
    "- control_input $u$\n",
    "- voltage_parametrisation $\\Xi$\n",
    "- result_state $\\hat{x}(t)$\n",
    "\n",
    "namely, all values that we need in the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model\n",
    "\n",
    "For the setup of the NN model, we need a few config parameters (number and size of the hidden layers and optionally a seed to control the intialisation of the NN's parameters). Additionally, we provide the component model as we will use it for the calculation of the physics loss $\\mathcal{L}_c$. Optionally, the training dataset can be used to set an internal normalisation of the NN.\n",
    "\n",
    "We aim to approximate the integration operation\n",
    "$\\begin{align}\n",
    "\\hat{x}(t) = x_0 + \\int_0^t f(x, u, \\Xi) dt\n",
    "\\end{align}$\n",
    "with the NN, hence the basic input-output structure the model should follow is $[t, x_0, u, \\Xi] \\mapsto \\hat{x}$. Calling `nn_model.forward()` with the corresponding inputs will predict the value $\\hat{x}$. Within the function `pinnsim.learning_functions.dynamical_system_NN.py`, we apply a few adjustments to a simple feed-forward neural network in order to help the learning process. In any case, the above mapping is always maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = setup_nn_model(\n",
    "    config=config,\n",
    "    power_system_model=component_model,\n",
    "    training_dataset=dataset_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser, scheduler, and loss function\n",
    "\n",
    "The remaining elements that we need to set up are\n",
    "- an optimiser, we use a L-BFGS optimiser as implemented in `torch`.\n",
    "- optionally schedulers for the learning rate and the weighting factor $\\alpha$ between the data $\\mathcal{L}_x$ and the physics loss $\\mathcal{L}_c$. The total loss is calculated as $\\mathcal{L} = \\mathcal{L}_x + \\alpha \\mathcal{L}_c$. In our experience, a too large value for $\\alpha$ can lead to problems during the first epochs of the training.\n",
    "- a loss function. The state $\\hat{x}$ has not the same units across all its variables, hence we define a scaling and compute the loss in the scaled/normed state space. These factors are component dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = setup_optimiser(nn_model=nn_model, config=config)\n",
    "\n",
    "learning_rate_scheduler, loss_weight_scheduler= setup_schedulers(\n",
    "    nn_model=nn_model, \n",
    "    optimiser=optimiser, \n",
    "    config=config   \n",
    ")\n",
    "\n",
    "loss_function = LossNormedState(component_model=component_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation function\n",
    "\n",
    "Next, we define the functions that constitute an epoch in the training process, namely a training step and an evaluation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "After unpacking the training dataset with simulated points (`dataset`) and the dataset with collocation points (`dataset_collocation`), we define a closure function that will be supplied to the optimiser. \n",
    "\n",
    "For the data points, we simply call `nn_model.forward()` and then apply the specified (scaled) loss function. \n",
    "\n",
    "For the collocation points, the function `nn_model.forward_lhs_rhs()` provides the\n",
    "- state prediction $\\hat{x}$\n",
    "- the temporal derivative of the state prediction `d_dt_state_prediction_c` $\\frac{d}{dt} \\hat{x}$\n",
    "- the update function $f$ evaluated with the state prediction `f_prediction_c` $f(\\hat{x}, u, \\Xi)$ \n",
    "\n",
    "The physics loss $\\mathcal{L}_c$ is calculated as $\\Vert \\frac{d}{dt} \\hat{x} - f(\\hat{x}, u, \\Xi) \\Vert^2$ and added to the data loss $\\mathcal{L}_x$ to form the total loss $\\mathcal{L} = \\mathcal{L}_x + \\alpha \\mathcal{L}_c$.\n",
    "\n",
    "As the loss values can become very small, we use a factor `loss_multiplier` to avoid being affected by the internal tolerance settings of the L-BFGS optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(\n",
    "    dataset,\n",
    "    dataset_collocation,\n",
    "    nn_model,\n",
    "    loss_function,\n",
    "    optimiser,\n",
    "):\n",
    "    nn_model.train()\n",
    "    time, state_initial, control_input, voltage_parametrisation, state_result = dataset\n",
    "    (\n",
    "        time_c,\n",
    "        state_initial_c,\n",
    "        control_input_c,\n",
    "        voltage_parametrisation_c,\n",
    "        _,\n",
    "    ) = dataset_collocation\n",
    "    loss_multiplier = torch.tensor(nn_model.epochs_total + 1)\n",
    "\n",
    "    def closure():\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        state_prediction = nn_model.forward(\n",
    "            time=time,\n",
    "            state_initial=state_initial,\n",
    "            control_input=control_input,\n",
    "            voltage_parametrisation=voltage_parametrisation,\n",
    "        )\n",
    "\n",
    "        loss_prediction = loss_function(inputs=state_prediction, targets=state_result)\n",
    "\n",
    "        state_prediction_c, d_dt_state_prediction_c, f_prediction_c = nn_model.forward_lhs_rhs(\n",
    "            time=time_c,\n",
    "            state_initial=state_initial_c,\n",
    "            control_input=control_input_c,\n",
    "            voltage_parametrisation=voltage_parametrisation_c,\n",
    "        )\n",
    "        \n",
    "        loss_physics = loss_function(\n",
    "            inputs=d_dt_state_prediction_c, targets=f_prediction_c\n",
    "        )\n",
    "\n",
    "\n",
    "        loss = (\n",
    "            loss_prediction + nn_model.physics_regulariser * loss_physics\n",
    "        ) * loss_multiplier\n",
    "        loss.backward()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    loss = optimiser.step(closure)\n",
    "\n",
    "    return loss / loss_multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation step\n",
    "\n",
    "The evaluation function simply evaluates the loss for the validation dataset. The NN model should be set to the evaluation mode `nn_model.eval()` in case that dropout or batch normalisation functions are used. The line `with torch.no_grad()` indicates, that no gradients are needed of the following function evaluation (unlike in the training); this speeds up the computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    dataset,\n",
    "    nn_model,\n",
    "    loss_function,\n",
    "):\n",
    "    nn_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (\n",
    "            time,\n",
    "            state_initial,\n",
    "            control_input,\n",
    "            voltage_parametrisation,\n",
    "            state_result,\n",
    "        ) = dataset\n",
    "\n",
    "        (\n",
    "            state_prediction,\n",
    "            d_dt_state_prediction,\n",
    "            f_state_prediction,\n",
    "        ) = nn_model.forward_lhs_rhs(\n",
    "            time=time,\n",
    "            state_initial=state_initial,\n",
    "            control_input=control_input,\n",
    "            voltage_parametrisation=voltage_parametrisation,\n",
    "        )\n",
    "    \n",
    "    loss_prediction = loss_function(inputs=state_prediction, targets=state_result)\n",
    "    loss_physics = loss_function(inputs=d_dt_state_prediction, targets=f_state_prediction)\n",
    "\n",
    "    return loss_prediction, loss_physics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "The training becomes a simple loop over `train_epoch` and `evaluate_model`. Optionally, we can add logging and/or printing functions and scheduled adjustment to certain parameters, e.g., the learning rate or the loss weighting parameter. If the training process is logged, it can be visualised afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_list = list()\n",
    "maximum_epochs = 20\n",
    "\n",
    "print(\"Epoch | Loss training | Loss prediction val| loss physics val \")\n",
    "\n",
    "while nn_model.epochs_total < maximum_epochs:\n",
    "    loss = train_epoch(\n",
    "                dataset=dataset_training[:],\n",
    "                dataset_collocation=dataset_collocation[:],\n",
    "                nn_model=nn_model,\n",
    "                loss_function=loss_function,\n",
    "                optimiser=optimiser,\n",
    "            )\n",
    "    \n",
    "    loss_prediction, loss_physics = evaluate_model(\n",
    "        dataset=dataset_validation[:],\n",
    "        nn_model=nn_model,\n",
    "        loss_function=loss_function,\n",
    "    )\n",
    "\n",
    "    learning_rate_scheduler.step()\n",
    "    loss_weight_scheduler()\n",
    "    print(\n",
    "        f\" {nn_model.epochs_total + 1:04} |\"\n",
    "        f\"      {loss:.2e} |\"\n",
    "        f\"           {loss_prediction:.2e} |\"\n",
    "        f\"         {loss_physics:.2e}\"\n",
    "    )\n",
    "    logging_list.append(torch.stack([torch.tensor(nn_model.epochs_total + 1), loss, loss_prediction, loss_physics]))\n",
    "    nn_model.epochs_total += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_log = torch.vstack(logging_list).detach()\n",
    "\n",
    "plt.plot(results_log[:, 0], results_log[:, 1], label=\"Training loss\")\n",
    "plt.plot(results_log[:, 0], results_log[:, 2], label=\"Validation loss data\")\n",
    "plt.plot(results_log[:, 0], results_log[:, 3], label=\"Validation loss physics\")\n",
    "plt.loglog()\n",
    "plt.legend()\n",
    "plt.xlim([1, maximum_epochs])\n",
    "plt.xlabel(\"Epoch [-]\")\n",
    "plt.ylabel(\"Loss [-]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing\n",
    "\n",
    "Finally, the model can be tested, e.g., evaluating the test loss. In `pinnsim.learning_functions.testing_functions.py`, we also compute the maximum and mean absolute error of each state to improve the interpretability of the learning outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(   \n",
    "    dataset,\n",
    "    nn_model,\n",
    "    loss_function\n",
    "):\n",
    "    nn_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (\n",
    "            time,\n",
    "            state_initial,\n",
    "            control_input,\n",
    "            voltage_parametrisation,\n",
    "            state_result,\n",
    "        ) = dataset\n",
    "\n",
    "        state_prediction = nn_model.predict(\n",
    "            time=time,\n",
    "            state_initial=state_initial,\n",
    "            control_input=control_input,\n",
    "            voltage_parametrisation=voltage_parametrisation,\n",
    "        )\n",
    "\n",
    "    loss_testing = loss_function(inputs=state_prediction, targets=state_result)\n",
    "    print(f\"Test loss {loss_testing:.2e}\")\n",
    "\n",
    "    \n",
    "test_model(\n",
    "            dataset=dataset_testing[:],\n",
    "            nn_model=nn_model,\n",
    "            loss_function=loss_function,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete workflow\n",
    "\n",
    "In `pinnsim.learning_functions`, we add more complexity to the above described setup and in `workflow.py`, there is an implementation of the entire workflow, such that it can be called from a single config file as shown below. An important aspect, that is added, is the logging functionality using [WandB](https://wandb.ai) and an automatic saving of the best model based on the validation data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinnsim.learning_functions.workflow import train\n",
    "sweep_config = default_hyperparameter_setup()\n",
    "run_config = convert_sweep_config_to_run_config(sweep_config=sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the `run_config` file the entire process can be called by the following line. The notebook `training_sweeps` shows how multiple models can easily be trained using this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(config=run_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
